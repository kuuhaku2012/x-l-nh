import gradio as gr
import cv2
import numpy as np
import tensorflow as tf
import os

# 1. C·∫§U H√åNH & LOAD MODEL
MODEL_PATH = "billboard_checkpoint.keras"
IMG_SIZE = (64, 64)
DEFAULT_THRESHOLD = 0.86

print("ƒêang t·∫£i model...")
try:
    model = tf.keras.models.load_model(MODEL_PATH)
    print("--> ƒê√£ t·∫£i model th√†nh c√¥ng!")
except Exception as e:
    print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file {MODEL_PATH}")
    exit()

# 2. H√ÄM X·ª¨ L√ù 
def preprocess_image(img_crop_rgb):
    """Chu·∫©n b·ªã ·∫£nh ƒë·ªÉ ƒë∆∞a v√†o model"""
    img_resized = cv2.resize(img_crop_rgb, IMG_SIZE)
    img_batch = np.expand_dims(img_resized, axis=0)
    return img_batch

def get_region_proposals(image_rgb):
    """T√¨m v√πng nghi ng·ªù b·∫±ng OpenCV"""
    # Gradio g·ª≠i ·∫£nh RGB, OpenCV c·∫ßn Gray ƒë·ªÉ x·ª≠ l√Ω
    gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
    edged = cv2.Canny(blur, 50, 150)
    contours, _ = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    proposals = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        if w < 30 or h < 30: continue
        aspect_ratio = w / float(h)
        if aspect_ratio < 0.2 or aspect_ratio > 5: continue
        proposals.append((x, y, w, h))
    return proposals

# 3. H√ÄM CH√çNH CHO GRADIO
def analyze_image(input_img, threshold):
    if input_img is None:
        return None, [], "Vui l√≤ng t·∫£i ·∫£nh l√™n."

    # Copy ·∫£nh ƒë·ªÉ v·∫Ω (·∫¢nh input c·ªßa Gradio l√† RGB)
    output_img = input_img.copy()
    
    # 1. T√¨m v√πng ƒë·ªÅ xu·∫•t
    proposals = get_region_proposals(input_img)
    
    detected_crops = [] # Danh s√°ch ch·ª©a ·∫£nh ƒë√£ c·∫Øt
    log_info = ""       # Chu·ªói ch·ª©a th√¥ng tin t·ªça ƒë·ªô
    count = 0

    # 2. Duy·ªát qua t·ª´ng v√πng
    for (x, y, w, h) in proposals:
        # C·∫Øt v√πng ·∫£nh (ROI)
        roi = input_img[y:y+h, x:x+w]
        
        # D·ª± ƒëo√°n
        processed_roi = preprocess_image(roi) # roi ƒë√£ l√† RGB
        prediction = model.predict(processed_roi, verbose=0)[0][0]
        
        # N·∫øu ƒë·∫°t ng∆∞·ª°ng tin c·∫≠y
        if prediction >= threshold:
            count += 1
            
            # A. V·∫Ω khung l√™n ·∫£nh l·ªõn
            cv2.rectangle(output_img, (x, y), (x + w, y + h), (0, 255, 0), 2)
            label = f"{prediction*100:.0f}%"
            cv2.putText(output_img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            # B. Th√™m v√†o danh s√°ch ·∫£nh crop (ƒë·ªÉ hi·ªán l√™n Gallery)
            # Th√™m ch√∫ th√≠ch cho ·∫£nh crop
            crop_label = f"Tin c·∫≠y: {prediction*100:.1f}%"
            detected_crops.append((roi, crop_label))
            
            # C. Ghi log th√¥ng tin (Format: class x y w h conf)
            log_info += f"Bi·ªÉn b√°o #{count}: x={x}, y={y}, w={w}, h={h}, Conf={prediction:.4f}\n"

    status_text = f"K·∫æT QU·∫¢: T√¨m th·∫•y {count} bi·ªÉn qu·∫£ng c√°o.\n\nCHI TI·∫æT:\n{log_info}"   
    return output_img, detected_crops, status_text

# 4. GIAO DI·ªÜN GRADIO
with gr.Interface(
    fn=analyze_image,
    inputs=[
        gr.Image(label="T·∫£i ·∫£nh ƒë√¥ th·ªã", type="numpy"),
        gr.Slider(0.5, 1.0, value=DEFAULT_THRESHOLD, label="ƒê·ªô tin c·∫≠y t·ªëi thi·ªÉu (Threshold)")
    ],
    outputs=[
        gr.Image(label="·∫¢nh ƒë√£ nh·∫≠n di·ªán"),
        gr.Gallery(label="C√°c bi·ªÉn b√°o ƒë√£ c·∫Øt (Crops)", columns=4, height=200), # Hi·ªÉn th·ªã ·∫£nh crop ƒë·∫πp m·∫Øt
        gr.Textbox(label="Th√¥ng tin chi ti·∫øt (Log)", lines=10)
    ],
    title="üîç Demo ƒê·ªì √Ån: Nh·∫≠n di·ªán Bi·ªÉn Qu·∫£ng C√°o",
    description="H·ªá th·ªëng Hybrid (OpenCV + CNN) t·ª± ƒë·ªông ph√°t hi·ªán, c·∫Øt v√† tr√≠ch xu·∫•t th√¥ng tin bi·ªÉn qu·∫£ng c√°o.",
    flagging_mode="never" 
) as demo:
    pass

if __name__ == "__main__":
    demo.launch()